<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8"> 
	<title>Alberto Gaona</title>
	<link href='./img/aglogo.ico' rel='icon' type='image/ico'/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
	
	<!-- For the landing -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script type="text/javascript" src="js/transition.js"></script> 

	<!-- For the project section -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap-theme.min.css">
  <script src="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>

  <!-- My styles -->
  <link rel="stylesheet" type="text/css" href="css/transition.css" />
  <link rel="stylesheet" type="text/css" href="css/riple.css">
  <link rel="stylesheet" type="text/css" href="css/style.css?v={random number/string}" />


</head>
<body>
	<!-- ********************************************* Landing page ********************************************* -->
	<div class="wrap" id="content-area">

      <video poster="./img/video.gif" id="bgvid" class="video-back" playsinline autoplay muted loop>
        <source src="./img/video.webm" type="video/webm">
        <source src="./img/video.mp4" type="video/mp4">
      </video>

    	<div class="animation">
        <!-- <img src="./img/testwild.png" class="animation-img"> -->
      </div>

    	<h1>Alberto Gaona</h1>
    	<h2>Computer Systems Engineer & "soon" also Artificial Intelligence Engineer</h2>

    	<button class="animated tada link"> See projects </button>	

    	<footer class="footer">
    		<div class="inner-container">
    			<ul class="footer-links">
    				<li><a href="https://www.facebook.com/betogaona07" target="_blank" rel="noopener"> Facebook </a></li>
    				<li><a href="https://www.github.com/betogaona7" target="_blank" rel="noopener"> Github </a></li>
    				<li><a href="https://www.linkedin.com/in/betogaona7/" target="_blank" rel="noopener"> LinkedIn </a></li>
    				<li><a href="https://www.instagram.com/betogaona7" target="_blank" rel="noopener"> Instagram </a></li>
    				<li><a href="https://betogaona7.github.io/blog" target="_blank" rel="noopener"> Blog </a></li>
    			</ul>
    		</div>

    		<div class="footer-copy">
    			© 2018 Alberto Gaona - <a href="mailto:albertoo_3c@hotmail.com"> Contact </a>
    		</div>
    	</footer>
	</div>

  <!-- ********************************************* Projects page ********************************************* -->
	<div id="content-2" style="display:none">
	    <div class="modal" id="trailer">
	      <div class="modal-dialog">
	        <div class="modal-content">
                <a href="#" class="hanging-close" data-dismiss="modal" aria-hidden="true">
                    <img src="https://lh5.ggpht.com/v4-628SilF0HtHuHdu5EzxD7WRqOrrTIDi_MhEG6_qkNtUK5Wg7KPkofp_VJoF7RS2LhxwEFCO1ICHZlc-o_=s0#w=24&h=24"/>
                </a>
                <div class="scale-media" id="technical-speech"></div>
	        </div>
	      </div>
	    </div>

	    <div>
	        <a href="" class=".link cleanlink"><h1>Alberto Gaona</h1></a>
	        <h2 style="color:#676767;">Projects</h2>
	    </div>

<div class="container">
<div class="row">

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="Mariya" 
technical-description="
  </p><div style='text-align: center;'>
    <video controls poster='./img/huicholdos.png'>
      <source src='./img/Mariya.mp4' type='video/mp4'>
    </video>
  </div><br/><br/><p>

  A Mexican project that opens a communication portal between an indigenous community and the rest of the world. 
  The Wixarikas erroneously known as Huicholes are part of a Mexican town. Globally known for their art: The 
  Huichol art.<br/> <br/>

  Do you know that every piece of art they create really tells a story? each bracelet, each table, each figure 
  tells something about their culture. Those stories create an identity for them. Through their tales, which have passed from 
  generation to generation, they create a heritage to encourage new generations to follow in the footsteps of their 
  ancestors.<br/> <br/>

  Apart from facing the possible disappearance of their sacred land due to mining, day by day they face a problem that goes 
  unnoticed: 'migration'. Being located in the highest part of the Sierra Madre Occidental, communication and access to 
  new technologies is almost impossible. This force the new generations to migrate to the big cities, confronting the 
  language problem that threatens the extinction of their native language.<br/> <br/>

  The Mariya project tries to demonstrate that the technology is not fought with the ancestral customs of a town, giving 
  the floor to those who deserve to be heard, seeking to rescue their stories and knowledge which they consider sacred.
  <br/> <br/>

  Fall in love with the Wixarika culture with the documentary: 
  <a target='_blank' rel='noopener' href='https://huicholesfilm.com/en/'>
  Huicholes the last peyote guardians</a>.
  <br/>Learn more about the Mariya project by visiting the 
  <a target='_blank' rel='noopener' href='https://proyectomariya.github.io/'> official website</a>.<br/><br/>
  " 
video-url="https://player.vimeo.com/video/195152250" 
github-url="https://github.com/proyectomariya/translate"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l0HU366rPnEgaLL2w/giphy.gif" class="project-img">
    <h3>Mariya</h3>
    <h4>Taking advantage of technology and all available resources, we intend to put an end to the communication barrier between the Wixarica (Huicholes) community and the rest of the world, seeking to rescue their stories and knowledge which they consider sacred. Mariya is a translation device that seeks to save a Mexican culture.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="Bike Share" 
technical-description="
  Bike sharing systems are a new generation of traditional bike rentals where the whole process of membership, rental and 
  return back has become automatic. Through these systems, the user is able to easily rent a bike from a particular position 
  and return back to another position. Currently, there are about over 500 bike-sharing programs around the world which are 
  composed of over 500 thousands bicycles.<br/><br/>

  This project is a simple Multi-Layer Neural Network to make predictions of bike-sharing usage. Using a 
  <a target='_blank' rel='noopener' href='https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset'>dataset</a> that has 
  the number of riders for each hour of each day from January 1, 2011, to December 31, 2012. The data is pretty complicated. 
  The weekends have lower overall ridership and there are spikes when people are biking to and from work during the week. 
  Also have information about temperature, humidity, and wind speed. all of this likely affect the number of riders. <br/>
  <br/>

  The network has two layers, a hidden layer, and an output layer. The hidden layer uses the sigmoid function for activations.
  The output layer has only one node and is used for regression, the output node is the same as the input node.<br/><br/>
  In the following graph we can compare our predictions with the actual data:<br/><br/>

  <div style='text-align: center;'>
    <img src='./img/bikeshare-graph.png' alt='Bike share results' class='bikeshare-img'>
  </div><br/><br/>
  " 
video-url="" 
github-url="https://github.com/betogaona7/Deep-Learning/tree/master/DLNF-BikeShare"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l0HUiSjiHbUO24oGQ/giphy.gif" class="project-img">
    <h3>Bike Share</h3>
    <h4>Imagine yourself owning a bike sharing company like CycleHop and you want to predict how many bikes you need because if you have too few you're losing money from potential riders or vice versa if you have too many you're wasting money on bikes that are just sitting around. This project predicts the number of bike-share users on a given day from historical data, so you can know how many bikes you will need in the near future.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="Sign Language Recognizer" 
technical-description="
  The overall goal of this project was to build a word recognizer for American Sign Language video sequences, demonstrating
  the power of probabilistic models. In particular, this project employs Hidden Markov Models (HMMs)  that use a 
  preprocessed 
  <a target='_blank' rel='noopener' href='https://www-i6.informatik.rwth-aachen.de/%7Edreuw/database-rwth-boston-104.php'>
  dataset</a> of tracked hand and nose positions extracted from the video to identify individual words from test sequences.<br/><br/>

  The next video is an example of how the hand locations are tracked:<br/><br/></p>

  <div style='text-align: center;'>
    <video controls>
      <source src='./img/Recognizer.mp4' type='video/mp4'>
    </video>
  </div><br/><br/><p>

  I experimented with four feature sets and three model selection methods (that is 12 possible combinations). The best 
  combination was features-ground (those are differences between hand and nose locations) with Selector CV (Log likelihood 
  with Cross-validation folds) that gets a Word Error Rate (WER) of 53%. This can be improved by adding techniques like 
  Context training, Statistical Grammar, State typing, Segmentally Boosted HMMs and probably many more that will help us 
  obtain even lower WER.<br/><br/>
  " 
video-url="" 
github-url="https://github.com/betogaona7/AI-projects/tree/master/AIND-Recognizer"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/xULW8BRAEkxshqm02Q/giphy.gif" class="project-img">
    <h3>Sign Language Recognizer</h3>
    <h4>According to the World Health Organization (WHO) 360 millon people worldwide have disabling hearing loss of which 70 million people use sign language as their first language. Now, how many interpreters are there? much less than half. In this project, I built a system that can recognize words communicated using the American Sign Language (ASL).</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="Air Cargo Planner" 
technical-description="
  In this project, I defined a group of problems in classical PDDL (Planning Domain Definition Language) that use a 
  planning search agent to solve deterministic logistics problems for an Air Cargo transport system. For example the next 
  problem:</br></br></p>

  <div style='text-align: center;'>
    <img src='./img/planner-problem.png' alt='Planner problem' class='planner-img'>
  </div><br/><br/><p>

  Then set up the problems for the search of two types: Non-heuristic searches and Heuristic searches. The non-heuristic searches
  include Breadth-First search (BFS), Depth-First graph search (DFS) and Uniform-Cost search (UCS). Of them, the most
  efficient was the UCS algorithm.</br></br>

  For the heuristic searches, I used the A star algorithm (A*) with two different heuristics:</br></p>
  <ul type='disk'>
    <li>
      <p>Ignore preconditions heuristic - The minimum number of actions that must be carried out from the current state 
      in order to satisfy all of the goal conditions by ignoring the preconditions required for an action to be executed.</p>
    </li>
    <li>
      <p>Level sum heuristic - The sum of level costs of the individual goals (admissible if goals independent).</p>
    </li>
  </ul></br><p>
  A* with ignore preconditions heuristic was the one that works best. The following image is the answer to the problem
  posed in the previous image solved by both algorithms (A* and UCS):<br/><br/></p>

  <div style='text-align: center;'>
    <img class='planner' src='./img/planner-solution.png' alt='Planner solution'>
  </div><br/><br/><p>

  UCS takes 157 seconds to solve it while A* only takes 30 seconds. That is one of the great advantages of using heuristics.<br/><br/>
  " 
video-url="" 
github-url="https://github.com/betogaona7/AI-projects/tree/master/AIND-Planning"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l0HUo260gjOsknsY0/giphy.gif" class="project-img">
    <h3>Air Cargo Planner</h3>
    <h4>Have you asked yourself how companies like DHL schedule all their shipments?, This project solves a group of problems for an Air Cargo transport system using a planning search agent, in which we see how an agent take advantage of the structure of a problem to construct a complex plan of action.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="TV Scripts Generator" 
technical-description="
  This project generates Simpsons TV scripts. Specifically for a scene in the Moes tavern, using a Recurrent Neural 
  Network (RNN) that was trained with a subset of the 
  <a target='_blank' rel='noopener' href='https://www.kaggle.com/wcukierski/the-simpsons-by-the-data'> Simpsons dataset 
  </a> of scripts from 27 seasons. For example: <br/><br/></p>

  <div style='text-align: center;'>
    <img src='./img/scripts-script.png' alt='Simpson script' class='scripts-img'>
  </div><br/><br/><p>

  It is okay if the TV script does not make much sense. The network was trained on less than a megabyte of text. In order 
  to improve results, we need to use a smaller vocabulary or get more data. Luckily there is more data! As mentioned in the 
  beginning, this is a subset of another database. Train on all the data would take too long. <br/><br/>
  " 
video-url="" 
github-url="https://github.com/betogaona7/Deep-Learning/tree/master/DLNF-Scripts"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l0HU7Ik9NfAnoWq9q/giphy.gif" class="project-img">
    <h3>TV Scripts Generator</h3>
    <h4>The way to write a script for a TV show is to actually write a script. A pen is useful, typing is also good. Keep putting words on the page, but if for some reason you don't feel good, you're sick or you just feel stuck, try to use a computer that writes it for you. This project generates an original piece of writing after being trained using your existing scripts, therefore, the new piece will conserve your own writing style.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="New Faces Generator" 
technical-description="
  This is a Generative Adversarial Network (GAN) that use the 
  <a target='_blank' rel='noopener' href='http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html'>CelebFaces Attributes dataset 
  (CelebA)</a> consisting of more than 200,000 of celebrity images with annotations as training to generate new images of 
  faces.<br/><br/>

  GANs are used for generating realistic data. The word adversarial in Generative Adversarial Networks means that we have 
  two networks, the generator and the discriminator which are in a competition with each other. The generator wants to 
  minimize the value function and the discriminator wants to maximize the value function.<br/><br/> 

  We can think of this process as being like a competition between counterfeiters and police. The generator net is like a 
  group of counterfeiters trying to produce fake money and pass it off as real. The police try to catch counterfeiters 
  using fake money but still want to let everyone else spend their real money. Over time the police get better at detecting 
  counterfeit money and the counterfeiters get better at faking it. Eventually, the counterfeiters are forced to make perfect 
  replicas of real money.<br/><br/>

  When we train a GAN on the CelebA dataset we can watch it being generating random images, gradually learn to generate 
  faces. That is the basic idea of how GANs work.<br/><br/>

  In the project, CelebA images were cropped to remove parts of the image that do not include a face, then resized down 
  to 28x28 pixels. The following image is an example of how the generated faces look.<br/><br/></p>

  <div style='text-align: center;'>
    <img src='./img/faces-example.png' alt='Faces'>
  </div><br/><br/><p>
  Note that some faces look like horror movie characters!<br/><br/>
  " 
video-url="" 
github-url="https://github.com/betogaona7/Deep-Learning/tree/master/DLNF-Faces"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l49JYD8R8W6RtOUBa/giphy.gif" class="project-img">
    <h3>New Faces Generator</h3>
    <h4>Imagine you work for a movie production company, and your job is to design the characters in a movie. You would usually draw a bunch of different concepts before arriving at the final design. What if you had a program that could do this for you?. This project generates new faces using celebrity images.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="The Isolation Game" 
technical-description="
  Game playing has long been one of the cornerstones of AI advancements. One of the biggest advancements in recent AI has 
  been in this field through Googles Go winning AlphaGo AI. Similarly, one of the seminal events of the 20th century was 
  seeing IBMs Deep Blue defeated Gary Kasparov, a world chess champion in a game of chess.<br/><br/>

  In this project, I designed and implemented a game-playing agent to play a game using adversarial search. The agent play 
  Isolation. A two-player game in which the players alternate turns moving a single piece from one cell to another on the 
  board. Whenever either player occupies a cell, that cell becomes blocked for the remainder of the game. The first player 
  with no remaining legal moves loses, and the opponent is declared winner. Each player is restricted to L-shaped movements 
  (like a knight in chess) on a 7 by 7 board, however, the player can jump blocked or occupied spaces (just like knight in 
  chess).<br/><br/>

  The goal was to program a player that beats its opponent consistently at this game. Additionally, the agent has a fixed 
  time limit each turn to search for the best move and respond. If the time limit expires during a players turn, that player 
  forfeits the match, and the opponent wins.<br/><br/>
  I used three different heuristics to perform the adversarial search and compare their performance:<br/>

  <ul type='disk'>
    <li>
      <p>The basic heuristic - Evaluate how good is the board for the player and how good it is for the opponent, then 
      subtract the opponents score from the players. This heuristics would penalize our computer player with more potential 
      moves which are counterproductive, It continues to take into account boards where the current player can make a larger 
      number of moves and also penalizes boards where the opponent can make a larger number of moves.</p>
    </li>
    <li>
      <p>The lucky heuristic - This heuristic take into account the number of locations that are still available on the 
      board, however this number does not reflect the goodness of the board but confirm the next statement: Both the number 
      of open spaces and the number of moves made are the same for every position for the same depth, thus any heuristic 
      that relies only on these two exactly as effective (minus very slight performance penalty) as a zero score function.</p>
    </li>
    <li>
      <p>The coward heuristic - It is called coward because the goal is to get as far away from the opponent as possible, here 
      is the otherwise of the basic heuristic evaluation function.</p>
    </li>
  </ul><br/><br/><p>

  In conclusion, the best was the basic heuristic because it obtains a better win rate, it is no computationally complex and 
  it evaluates very well how the current board configuration is for our player.<br/><br/></p>

  <div style='text-align: center;'>
    <img src='./img/isolation.gif' alt='Isolation gif' class='isolation-img'>
    </div><br/><br/><p>
    " 
video-url="" 
github-url="https://github.com/betogaona7/AI-projects/tree/master/AIND-Isolation"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/3o752fb9nnFM3P7JOo/giphy.gif" class="project-img">
    <h3>The Isolation Game</h3>
    <h4>Isolation is a two-player game in which the players alternate turns moving a single piece from one cell to another on a board. Whenever either player occupies a cell, the cell becomes blocked for the remainder of the game. The first player with no remaining legal moves loses, and the opponent is declared the winner. In this version, each player is restricted to L-shaped movements on a 7x7 board. This project is a game-playing agent to win the isolation game.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="The Sudoku Puzzle" 
technical-description="
  This project can solve any Sudoku puzzle, also have some extensions. The first extension is an implementation of the 
  naked twins technique. The second is a modification to solve diagonal sudokus. <br/><br/>

  The naked twins technique is the following. Consider the following puzzle, and look the two highlighted boxes, F3 
  and I3.<br/><br/></p>

  <div style='text-align: center;'>
    <img class='sudoku-img' src='./img/naked-twins.png' alt='Naiked twins 1'>
  </div><br/><br/><p>

  As we can see, both belong to the same column, and both permit the values 2 and 3. Now, we do not know which one has 
  a 2 and which one has a 3, but we know one thing for sure - the values 2 and 3 are locked in those two boxes, so no 
  other box in their same unit (the third column) can contain the values 2 or 3.<br/><br/> Thus, we go over all the 
  boxes in the same unit, and remove the values 2 and 3 from their possible values.<br/><br/></p>

  <div style='text-align: center;'>
    <img class='sudoku-img' src='./img/naked-twins-2.png' alt='Naiked twins 2'>
  </div><br/><br/><p>

  As you can see, we've removed the values 2 and 3 from the boxes D3 and E3. This is naked twins technique. A diagonal 
  sudoku is like a regular sudoku, except that among the two main diagonals, the number 1 to 9 should all appear exactly 
  once. The next video is an example where the agent solves a diagonal sudoku.<br/><br/></p>

  <div style='text-align: center;'>
    <video controls class='sudoku-vid'>
      <source src='./img/Sudoku.mp4' type='video/mp4'>
    </video>
  </div><br/><br/><p>
  " 
video-url="" 
github-url="https://github.com/betogaona7/AI-projects/tree/master/AIND-Sudoku"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/3o751YzOR1rXv8FVTO/giphy.gif" class="project-img">
    <h3>The Sudoku Puzzle</h3>
    <h4>Surely you have played Sudoku at least once in your life, Sudoku is that puzzle game that comes in magazines, newspapers, etc. The goal is to fill a 9x9 grid with digits so that each column, each row, and each of the nine 3x3 subgrids that compose the grid contains all of the digits from 1 to 9. Have you ever been stuck trying to solve one?. This project is a play-game agent for solve diagonal sudoku puzzles.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="Image Classification" 
technical-description="
  Since humans started imagining machines moving around and interacting with the world, we have given them eyes. Look at
  <a target='_blank' rel='noopener' href='https://www.youtube.com/watch?v=ARJ8cAGm6JE'>HAL9000</a> from 2001: A Space 
  Odyssey. He is just one big eye.<br/><br/>

  Today, we want automated cars to be able to distinguish between people, and trees, and the road. We are enabling our 
  cars to do this using Convolutional Neural Networks (CNNs).<br/><br/>

  Starting with the work of Yann LeCun in the late 90s, convolutional networks, typically shortened to cognets, gained 
  popularity for detecting and recognizing handwritten characters and images. In 2012 a huge dataset called ImageNet 
  was released with 1,000 labeled categories and over a million training images. Alex Krizhevskys AlexNet, used a deep 
  convolutional network trained on GPUs to achieve a 15% error rate on ImageNet, and this is easily beat the second best
  attempt that had 26% errors.<br/><br/>

  In this project, I built a Convolutional Neural Network with TensorFlow and it recognizes objects and images. Here I used 
  the <a target='_blank' rel='noopener' href='https://www.cs.toronto.edu/~kriz/cifar.html'>CIFAR-10 dataset</a> which 
  consists of 60,000 images of ten different objects. This dataset is commonly used in computer vision research.<br/><br/>

  The following image is an example of the predictions made by the network.<br/><br/></p>
  <div style='text-align: center;'>
    <img src='./img/classifier.png' alt='CNN results' class='classification-img'>
  </div><br/><br/><p>
  " 
video-url="" 
github-url="https://github.com/betogaona7/Deep-Learning/tree/master/DLNF-Classification"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l0HU1QXrScjR6PUdi/giphy.gif" class="project-img">
    <h3>Image Classification</h3>
    <h4>Vision is the main way humans gain information about the world, with the passage of time we have been able to give eyes to the machines, from that, amazing applications have been created. For example, a company called Orbital Insigths  analyze satellite imagery to count cars and oil tank levels automatically to predict such things as mall sales and oil production. In this project, I developed a program capable of recognizing and classify ten different types of images.</h4>
    <h5>Read more...</h5>
</div>

<div class="col-md-6 col-lg-4 movie-tile text-center size" 
project-name="Machine Translation" 
technical-description="
  Take a peek into the realm of neural network machine translation. In this project, I taught a sequence-to-sequence 
  (seq2seq) model how to translate from one language to another training it on a dataset of English to French sentences.
  <br/><br/>

  Recurrent Neural Networks (RNNs) are a powerful class of neural networks that deal with sequential data. They are
  especially suited for language and translation tasks because they can extend to sequences of any length. But more 
  importantly, they share their parameters across different timesteps. So when they do learn a language model, they do it 
  a lot more efficiently than a traditional feedforward network would.<br/><br/>

  The challenge with neural networks is to find the right dataset to feed your model and guide it through the learning 
  process.<br/><br/>

  The seq2seq model can do amazing things. If you have a dataset with questions and answers you can make a 
  Question-Answering model, or if you have a dataset with new articles and their summaries you can make a summarization bot, 
  or like in this project, if you have a dataset with English phrases and French phrases you can make an English to French 
  translator.<br/><br/></p>

  <div style='text-align: center;'>
    <img src='./img/traductor-example.png' alt='English to French sentence' class='translation-img'>
    </div><br/><br/><p>

    Since the dataset I used only has a vocabulary of 227 English words of thousands that you use, you are going to see 
    good results using these words. If you want to create a better translation model, you will need better data. You can train 
    on the <a target='_blank' rel='noopener' href='http://www.statmt.org/wmt10/'>WMT10 French-English corpus dataset</a>. 
    This dataset has more vocabulary and richer in topics discussed, however, this will take you days to train, so make sure 
    you have a GPU and the neural network is performing well on this dataset. <br/><br/>
    " 
video-url="" 
github-url="https://github.com/betogaona7/Deep-Learning/tree/master/DLNF-Translation"
data-toggle="modal" data-target="#trailer">
    <img src="https://media.giphy.com/media/l0HU3pdWVTEz8qPCM/giphy.gif" class="project-img">
    <h3>Machine Translation</h3>
    <h4>Language is the backbone of our civilization. Without written records of previous scientific discoveries, we could never accomplish great events like traveling to space. We've accomplished a lot in the world where 13 of the most common languages are natively spoken by less than 50 percent of the population. What would the world be like if we could all work together without a language barrier?. This project is an English to French translator.</h4>
    <h5>Read more...</h5>
</div>

</div>
</div>
	<footer class="footer">
      <div class="inner-container">
        <ul class="footer-links">
          <li><a class="color-nol" href="https://www.facebook.com/betogaona07" target="_blank" rel="noopener"> Facebook </a></li>
          <li><a class="color-nol" href="https://www.github.com/betogaona7" target="_blank" rel="noopener"> Github </a></li>
          <li><a class="color-nol" href="https://www.linkedin.com/in/betogaona7/" target="_blank" rel="noopener"> LinkedIn </a></li>
          <li><a class="color-nol" href="https://www.instagram.com/betogaona7" target="_blank" rel="noopener"> Instagram </a></li>
          <li><a class="color-nol" href="https://betogaona7.github.io/blog" target="_blank" rel="noopener"> Blog </a></li>
        </ul>
      </div>

      <div class="footer-copy color-nol" style="margin-bottom: 15px;">
        © 2018 Alberto Gaona - <a href="mailto:albertoo_3c@hotmail.com"> Contact </a>
      </div>
  </footer>	
</div>

<div class="ripple-wrap"><div class="ripple"></div>

<script type="text/javascript" charset="utf-8">
  $(document).on('click', '.hanging-close, .modal-backdrop, .modal', function (event) {
    $("#trailer-video-container").empty();
  });
  $(document).on('click', '.movie-tile', function (event) {
    var projectName = $(this).attr('project-name');
    var technicalDescription = $(this).attr('technical-description');
    var videoUrl = $(this).attr('video-url');
    var codeUrl = $(this).attr('github-url');
    $('#technical-speech').empty().append($(
      '<h1 style="margin-bottom: 30px;" class="project-popup-title">' + projectName + '</h1>' +
      '<p>' + technicalDescription + '</p>' +
      '<a class="popuplink" target="_blank" rel="noopener" href="' + codeUrl + '">Fork in GitHub</a>'
      ,{
    }));
  });
  $(document).ready(function () {
    $('.movie-tile').hide().first().show("fast", function showNext() {
      $(this).next("div").show("fast", showNext);
    });
  });
</script>

</body>
</html> 